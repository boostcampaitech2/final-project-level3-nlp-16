import re

from transformers import AutoTokenizer, AutoModelForCausalLM
import time

MODEL = "nlprime/hash-tag-generator"


def main():

    tokenizer = AutoTokenizer.from_pretrained(MODEL, use_auth_token=True)

    model = AutoModelForCausalLM.from_pretrained(MODEL, use_auth_token=True)

    title = "íŒŒì†í°ë§¤ì… ì¤‘ê³ í°ë§¤ì… ì•„ì´í°12 ì•„ì´í°11 ì•„ì´í°xs ì•„ì´í°x"
    text = "ğŸ‘‰ ì¤‘ê³ í° íŒŒì†í° íí° íƒœë¸”ë¦¿ ì „ë¶€ ë§¤ì…í•©ë‹ˆë‹¤  ğŸ‘ˆ ğŸ‘‰ í—ˆìœ„ê°€ê²©ì œì‹œ.ë¶ˆí•„ìš”í•œ ì°¨ê° ì ˆëŒ€ í•˜ì§€ì•Šì•„ìš” ğŸ‘ˆ ğŸ‘‰ ë¶€í‰ì§€í•˜ìƒê°€ ë§¤ì¥ & ë²ˆê°œì¥í„° 7ë…„ì°¨ ìš´ì˜ì¤‘ì´ì—ìš” ğŸ‘ˆ ğŸ˜ 24ì‹œê°„ í†¡ . ì „í™” . ë¬¸ì ì–¸ì œë“  ìƒë‹´ í™˜ì˜í•´ìš” ğŸ˜ ğŸ˜ ì•„ì´í°ì‚¬ì„¤ìˆ˜ë¦¬ì ë„ ìš´ì˜í•˜ê³  ìˆìœ¼ë‹ˆ . ì™„íŒŒëœ í°ë„ ì „ë¶€ ì ê²€ë° ë§¤ì… ê°€ëŠ¥í•©ë‹ˆë‹¤ ğŸ˜ğŸ˜ ë¶€í‰ì•„ì´í´ë¦¬ë‹‰ ğŸ˜ ê²€ìƒ‰í•´ì„œ ë³´ì‹œë©´ ë°©ë¬¸ì ì‹¤ë¦¬ë·°ë° ë§¤ì¥ í™•ì¸ê°€ëŠ¥í•˜ì˜¤ë‹ˆ ì•ˆì‹¬í•˜ì…”ë„ ë©ë‹ˆë‹¤. ğŸ¤© ê±°ë˜ë°©ë²• > ì§ê±°ë˜ì‹œ ë§¤ì¥ë°©ë¬¸ í•´ì£¼ì‹œë©´ ë˜ì‹œë©°.. ë˜ëŠ” íƒë°°ê±°ë˜ ì§„í–‰í•˜ê³ ìˆìŠµë‹ˆë‹¤. íƒë°°ê±°ë˜ì‹œ ìƒì  ì°œ íŒ”ë¡œìš°ì‹œ íƒë°°ë¹„ 3ì²œì› ì§€ì›í•´ë“œë¦¬ê³  ìˆìŠµë‹ˆë‹¤ ğŸ¤© ğŸ‘‰ íƒë°°ê±°ë˜ì‹œ ì¼€ì´ìŠ¤.í•„ë¦„.ë°•ìŠ¤ ì „ë¶€ íê¸°í•˜ì˜¤ë‹ˆ ì°¸ê³ í•´ì£¼ì„¸ìš”ğŸ‘ˆ âœŒ ë§¤ì¥ë°©ë¬¸ > ë¶€í‰ì—­ì§€í•˜ìƒê°€ ë¶„ìˆ˜ëŒ€ ì˜¤ì…”ì„œ ì—°ë½ì£¼ì‹œë©´ ë¹ ë¥´ì„¸ìš”âœŒ íƒë°°ê±°ë˜ > ì¸ì²œ ë¶€í‰êµ¬ ê´‘ì¥ë¡œ16 ë¯¼ìì—­ì‚¬ì‡¼í•‘ëª° ì§€í•˜2ì¸µ20í˜¸ ë§¤ì£¼ í™”ìš”ì¼ì€ íœ´ë¬´ì´ì˜¤ë‹ˆ ì°¸ê³ ë¶€íƒë“œë¦¬ê² ìŠµë‹ˆë‹¤ ğŸ‘Š ìƒí˜¸ > ë‘ë¦¬ëª¨ë°”ì¼ ğŸ‘Š ğŸ‘Š  íƒë°°ê±°ë˜ì‹œ ì•¡ì •ë°ê¸°ëŠ¥ì´ìƒ ìœ ë¬´ ê¼­ í™•ì¸ë¶€íƒë“œë¦´ê»˜ìš” ğŸ¤—"

    ids, max_len = cleaning(title, text, tokenizer)
    exit_word = ""
    while exit_word != "exit":
        start = time.time()
        result = inference(ids, max_len, model, tokenizer)
        print("Hash tag result : ", result)
        end = time.time()
        print(end - start)
        exit_word = input("ìƒì„±ëœ hash tagë¥¼ ì´ìš©í•˜ì‹œë ¤ë©´ exitë¥¼ ì…ë ¥í•˜ì„¸ìš”")


def cleaning(title, text, tokenizer):
    # title
    title = re.sub(r"[^\w\s]", " ", title)

    # text
    text = text.replace("\n", " ")
    text = text.replace("\r", " ")
    text = re.sub(r"[^\w\s]", "", text)
    input_line = "<s>" + title + "<sep>" + text + "<sep>"

    input_ids = tokenizer.encode(
        input_line, add_special_tokens=True, return_tensors="pt"
    )

    return input_ids, len(input_line)


def inference(input_ids, max_len, model, tokenizer):

    output_sequence = model.generate(
        input_ids=input_ids,
        do_sample=True,
        max_length=max_len,
        num_return_sequences=1,
        top_p=0.1,
    )

    decode_output = tokenizer.decode(output_sequence[0], skip_special_tokens=True)
    decode_output = decode_output[max_len + 1 :]
    decode_output = decode_output.split(",")
    decode_output = list(set(decode_output))
    decode_output = list(filter(None, decode_output))
    return decode_output


if __name__ == "__main__":
    main()
